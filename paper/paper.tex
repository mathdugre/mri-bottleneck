\documentclass[conference]{IEEEtran}

\usepackage{cite}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{hyperref}
\hypersetup{colorlinks=true,linkcolor=black,citecolor=blue,filecolor=black,urlcolor=blue}
\graphicspath{{figures}}
\usepackage{caption}
\usepackage{subcaption}
\captionsetup[subfigure]{singlelinecheck=false, skip=0pt}
\usepackage{listings}
\renewcommand{\lstlistingname}{Listing}
\usepackage[binary-units=true]{siunitx}
\usepackage{csvsimple}
\usepackage{booktabs}
\usepackage{amsmath}

\usepackage{color,soul}
\newcommand{\TG}[1]{\color{blue}\textsc{From Tristan: }#1\color{black}}
\newcommand{\MD}[1]{\color{magenta}\textsc{From Mathieu: }#1\color{black}}
\newcommand{\HL}[1]{\hl{#1}}

\title{An Analysis of Performance Bottlenecks in MRI Pre-Processing}
\author{Mathieu Dugr\'e, Yohan Chatelain, Tristan Glatard\\Department of Computer Science and Software Engineering, Concordia University, Montreal, QC, Canada}

\begin{document}
\maketitle

\begin{abstract}
	Magnetic Resonance Image (MRI) pre-processing is a critical step for neuroimaging analysis. However, the computational cost of MRI pre-processing pipelines is a major bottleneck for large cohort studies and some clinical applications. While High-Performance Computing (HPC) and, more recently, Deep Learning have been adopted to accelerate the computations, these techniques require costly hardware and are not accessible to all researchers. Therefore, it is important to understand the performance bottlenecks of MRI pre-processing pipelines to improve their performance. Using Intel VTune profiler, we characterized the bottlenecks of several commonly used MRI-preprocessing pipelines from the ANTs, FSL, and FreeSurfer toolboxes. We found that few functions contributed to most of the CPU time, and that linear interpolation was the largest contributor. Data access was also  a substantial bottleneck. We identified a bug in the ITK library that impacts the performance of ANTs pipeline in single-precision and a potential issue with the OpenMP scaling in FreeSurfer recon-all. Our results provide a reference for future efforts to optimize MRI pre-processing pipelines.
\end{abstract}

\section{Introduction}
% Computational bottleneck
Pre-processing of Magnetic Resonance Imaging (MRI) data is a critical step in neuroimaging. Although established pre-processing pipelines exist, they commonly require extensive amounts of computation and produce large volumes of output and intermediate data. Such requirements create challenges when studying large neuroimaging cohorts, and they limit clinical applicability when timely data analysis is needed. Therefore, the neuroimaging community is constantly seeking for ways to improve the computational performance of MRI pre-processing pipelines. In this paper, we characterize the computational cost of several commonly adopted MRI pre-processing pipelines, providing a reference benchmark for future efforts to optimize MRI pre-processing pipelines.

% % HPCs
% Researchers broadly adopted High Performance Computing (HPC) to accelerate the MRI processing of large cohorts \HL{leveraging} a large number of CPU cores and high-bandwidth memory to process subjects in parallel. However, \HL{it can be challenging to run HPCs in developing countries due to power restrictions, national network infrastructure, and lack of training and funding}~\cite{NRG-SC23}. \HL{Similarly, researcher use cloud computing, but it requires expertise to setup, can be costly for large studies, and} data transfer can be slow and might be impossible due to ethic concerns related to human data. \HL{In both cases, limitation from the parallel efficiency of the pipeline can lead to under-utilization of the available resources.}

% % DL
% In recent years, Deep Learning (DL) has shown promising results in MRI pre-preprocessing. For example, tools such as SynthMorph~\cite{Hoffmann2022-hu}, SynthSeg~\cite{Billot2023-vp}, SynthSR~\cite{Iglesias2023-bp}, and SynthStrip~\cite{Hoopes2022-ms} were integrated into FreeSurfer~\cite{Fischl2012-cx} to provide Deep Learning alternatives to traditional algorithms\HL{, providing faster runtime.}
% % Moreover, a study~\cite{Pepe2023-dm} showed that the results generated by SynthMorph are numerical more stable than those from FreeSurfer.
% However, training a DL model requires large amount of data and specialized hardware, and is computationally expensive. Additionally, \HL{their results} can be challenging to interpret, explaining the slow adoption of DL in MRI pre-processing.

% % Alternative solutions
% \HL{These techniques require costly hardware to be effective, which can be unacessible to underrepresented researchers.} Alternatively, improved algorithm designs, compression techniques, or reduced- and mixed-precision techniques could provide substantial performance improvements but require substantial effort to implement. Therefore, it is critical to understand the performance bottleneck of MRI pre-processing pipelines to improve their performance.

% - No comprehensive study of the performance bottleneck of MRI pre-processing.

% - Focus on MRI pre-processing.
MRI is a standard tool used by neuroscientists to perform clinical diagnosis and for researchers to develop a better understanding of the brain. Three main MRI modalities exist: structural MRI (sMRI), functional MRI (fMRI), and diffusion MRI (dMRI). While other modalities such as electroencephalography (EEG), computed tomography (CT), and positron emission tomography (PET) exist, we focus on MRI for its broad adoption in research and non-invasiveness. Moreover, MRI data analysis is challenging due to computationally expensive pre-processing and large output and intermediate data size.

% - Pipelines
Neuroscientists developed various toolboxes to tackle the challenging task of pre-processing MRI data. We focus on the commonly accepted fMRIPrep~\cite{Esteban2019-bl} pipeline as it a comprehensive pre-processing application for both sMRI and fMRI. fMRIPrep combines several pipelines to produce a complete pre-processing pipeline. We study fMRIPrep's sub-pipelines separately, to provide a finer grained analysis of the performance bottlenecks.

% - VTune profiler
To profile the pipelines, we use the Intel VTune tool~\cite{vtune-profiler}, a multi-language profiler with low performance overhead that provides runtime information at the level of functions. We profile the pipeline in single-threaded mode and using 32 threads to account for parallel executions. We aggregate subjects' results for each pipeline to obtain an average performance profile. 

\section{Materials \& Methods}
\subsection{Pipelines}
fMRIPrep is a commonly used fMRI and sMRI pre-processing pipeline for neuroimaging, built on top of several well-known neuroimaging toolboxes such as ANTs, FSL, FreeSurfer and AFNI \TG{expand acronyms and add references for each toolbox}. Although processing steps may vary based on the type of input data, the sMRI pipeline generally uses ANTs BrainExtraction~\cite{Tustison2010-gg,Avants2008-ea} to perform intensity correction and skull-stripping of a T1 weighted image, FSL FAST~\cite{Zhang2001-hx} to segment the tissues of the extracted brain, and ANTs registrationSyN~\cite{Avants2008-ea} to register the segmented brain. When enabled, FreeSurfer recon-all~\cite{Dale1999-wu} is used for surface reconstruction. We benchmarked a recon-all command similar to  fMRIPrep's, which (1) performs autorecon1 without skull-stripping, (2) imports an external skull-strip from ANTs brainExtraction, and (3) resumes reconstruction with autorecon2 and autorecon3. The fMRI pipeline generally performs motion correction with FSL MCFLIRT~\cite{Jenkinson2002-od}, slice timing correction with AFNI 3dtshift~\cite{Cox1996-nk}, and co-registration with FSL FLIRT~\cite{Jenkinson2002-od,Jenkinson2001-eu,Greve2009-dw}.

We profiled the entire fMRIPrep pipeline to obtain a coarse view of the pipeline's performance bottleneck. Additionally, to characterize the performance at a finer grain, we profiled the sub-pipelines of fMRIPrep, namely \TG{add list}. While we used the same sub-pipeline and order as in fMRIPrep, we used the default parameters for most of the sub-pipelines which may vary from the default parameters of fMRIPrep. We omitted some sub-pipelines of fMRIPrep as they were not compatible with our dataset. For example, slice-timing correction was already performed in our data.

ANTs brainExtraction and registrationSyN are available in single or double precision, leveraging the Insight Segmentation and Registration Toolkit (ITK)~\cite{Yoo2002-ve}. We profiled both versions to understand the performance benefits of using reduced precision in the pipelines.

\subsection{Data}
To ensure that our performance profiles are inclusive of different populations, we used data with a wide range of age and equal distribution of sex. We used the OpenNeuro ds004513 v1.0.2 dataset~\cite{ds004513:1.0.2}. The anatomical, functional, and diffusion data from 20 health individuals was acquired from two different cohorts. The within-subject cohort has nine participants (mean age=43~yrs, std=7~yrs; 4 females) with two sessions: eyes open and eyes closed. The replication cohort has eleven participants (mean age=27~yrs, std=5~yrs; 6 females) with only the eyes open session. \TG{add information on image resolution}

\subsection{Profiling}
Profiling MRI pre-processing pipelines raises a few challenges. Neuroimaging pipelines use several programming languages including \textit{C}, \textit{C++}, \textit{Fortran}, \textit{Matlab}, and \textit{Python}, with several pipelines utilizing a combination of multiple languages. Pipelines are generally complex and computationally expensive.

The Intel VTune profiler addresses these challenges by offering multi-language support, low performance overhead, and information of functions at runtime level. However, common profiling challenges remain. Profilers require source code to be compiled with debug symbols to report interpretable information about the different functions and modules. Moreover, different input data can substantially impact the performance results due to conditional branching in the pipeline and convergence thresholds. We discuss our approach to these challenges in the next section.
\MD{TODO Cite effect from varying input data on numerical stability and reduced precision. Ask yohan for references}
		
First, to obtain human-readable information on function and module names in VTune, we re-compiled each pipeline with debug information in a Docker image. For use on HPC systems, we created Apptainer~\cite{Kurtzer2017-bu} images using Docker2Singularity\footnote{\href{https://github.com/singularityhub/docker2singularity}{https://github.com/singularityhub/docker2singularity}}. \lstlistingname~\ref{lst:vtune_example} illustrates the profiling of a pipeline by mounting the VTune binary to the Apptainer image during execution.

\begin{minipage}{\linewidth}
	\lstinputlisting[
		language={sh},
		caption={Profiling a pipeline within an Apptainer container, using VTune profiler.},
		captionpos=b,
		label=lst:vtune_example,
		numbers=left,
		frame=single,
		basicstyle=\footnotesize,
		numbersep=5pt,
		numberstyle=\tiny\color{gray},
		rulecolor=\color{black},
		tabsize=2,
	]{vtune-example.sh}
\end{minipage}
			
We profiled the pipeline using two threading configuration: single-threaded and 32 threads. The single-threaded approach simplified the analysis and limited the potential I/O overhead on the profiling. 
The 32-thread approach used all available CPU cores from a compute node of our infrastructure to study the multi-threading performance of the pipeline. This configuration provided a more realistic usage of the pipeline.
			
Before profiling each pipeline, we transferred the input data from the shared file system to the compute node. After profiling, we transferred back the output to the shared file system. This limited profiling variability due to I/O contingency.
			
The data generated by the VTune profiler was written to a shared file system, to simplify post-processing. In principle, this could have increased the profiling overhead. However, in practice there was no impact since the data generated by the VTune profiler was very small and was not written on the same file system as data produced by the pipelines.
			 
\subsection{Metrics Definition}
VTune provides several metrics to characterize the performance of a pipeline.  The makespan is the elapsed time from the start to the end of the pipeline. The CPU time refers to the total time spent by the CPU, across all cores, to execute the pipeline. 

The memory bound metrics counts the number of cycles where the CPU is starved while waiting for in-flight memory demand load. L1 bound occurs when a demand load stalls CPU cores although the data already reside in L1 cache. Similarly, L2, L3, and DRAM bound occurs when a demand load stalls the CPU and there is a cache miss in the previous level of cache, but the data reside in the L2, L3, and DRAM respectively. The store bound occurs when a store operation stalls the CPU. Supplementary Figure~\ref{fig:memory-metrics} shows the monitoring events and equation to derive the metrics defining the memory bound characterization~\cite{Intel2006-lc}. We use the convention from~\cite{Kukunas2015-jd} for the equations.
			
The difference between the memory bound value and the sum of the other metrics represent the percentage of CPU starvation while waiting from data absent from all cache level, that is, when the data is being fetched from disk.
			
\subsection{Infrastructure}
We used the \textit{Slashbin cluster} at Concordia University. The compute nodes are configured with two 16-core Intel(R) Xeon(R) Gold 6130 CPU @ \SI{2.10}{\giga\hertz}, \SI{250}{\gibi\byte} of RAM, \SI{126}{\gibi\byte} of tmpfs, six \SI{447}{\gibi\byte} SSDs with the XFS file system (no RAID enabled), Rocky~Linux~8.9 and Linux kernel \textit{4.18.0-477.10.1.el8\_lustre.x86\_64}.
			
\section{Results \& Discussion}
In this section we present the aggregated profiling data obtained with VTune. First, we present a performance overview of the pipelines. We then analysis specific hotspots in the pipelines. Unless specified explicitly, we only present the profile data from using 1-thread since the results from 32-threads are similar.

We aggregate the profiling results of each pipeline across all subjects to obtain an average performance profile. Consistently with~\cite{Kukunas2015-jd}, we hypothesized that the Pareto principle applied to the performance of MRI pre-processing pipelines, we therefore only studied the functions with the largest contribution to CPU time, up to 80\% cumulative total runtime of the pipeline. This heuristic allowed us to focus our efforts on the performance critical sections of a pipeline. We report the mean and standard deviation of the execution time for those functions.
			
\subsection{CPU time distribution across functions was long-tailed}
Figure~\ref{fig:long-tail-distribution} shows that 80\% of the total CPU time was spent in less than 1.5\% of the function across all pipelines. While consistent with the Pareto principle, the distribution of the functions' CPU time did not follow a Pareto distribution. Using the \textit{powerlaw} Python package~\cite{Alstott2014-gr}, we found that the data fit a Zipf law with exponent $\alpha=1.46$ (Supplementary Figure~\ref{fig:zips-law}). Therefore, the CPU time of a function was approximately inversely proportional to its rank. Thus, future efforts to optimize this minimal set of functions could result in significant performance improvements.

\begin{figure}[ht]
	\centering
	\includegraphics[width=\linewidth]{figures/global-longtail.png}
	\caption{Distribution of the functions' CPU time. The left y-axis shows the average total CPU Time spent in a function, while the right y-axis show the cumulative CPU time percentage. The x-axis is the percentage of functions ordered by decreasing CPU time. The data includes all functions from all pipelines. }
	\label{fig:long-tail-distribution}
\end{figure}

\subsection{Main Bottleneck: Linear Interpolation}
Table~\ref{extab:interpolation} shows that interpolation was a critical part for all pipelines except FreeSurfer recon-all, contributing between 32.20\% to 62.70\% of the total CPU time. Overall, the number of functions using interpolation was low, with less than 20 in each pipeline: few interpolation functions contributed to a large portion of the CPU time. While each interpolation had low computational cost, the large amount of operations resulted in a significant bottleneck. Therefore, optimizing interpolation would bring substantial benefits to these pipelines.

\csvnames{csvcol}{1=\pipeline, 2=\nfunc, 3=\cputime}
\csvstyle{interpolation}{
	tabular = |r|c|c|,
	table head = \hline Pipeline & \# of functions & \% CPU Time \\
	& with interpolation & spent in interpolation\\\hline\hline,
	late after line = \\\hline,
	respect all,
	csvcol
}
\begin{table*}[ht]
	\centering
	\csvreader[interpolation]{tables/interpolation.csv}{}
	{\pipeline & \nfunc & \tablenum[round-precision=2]{\cputime}}
	\caption{Contribution of interpolation to the pipelines' total CPU time. The percentage is the average sum of CPU time of functions using interpolation. The data includes all functions from all pipelines.}
	\label{extab:interpolation}
\end{table*}



\subsection{Impact of Memory Bounded Functions}
Table~\ref{extab:memory-bound} shows that fetching data from disk was the main cause for starving the CPU \TG{add data fetching time \% to table}. Next, the L1 bound had the highest cache-level contribution to the memory bound of the pipelines. ANTs pipelines had the highest L1 bound, followed by FreeSurfer recon-all and FSL MCFLIRT. L2 and L3 bound was low for all pipelines, although higher for FreeSurfer recon-all. The DRAM bound was high for both FSL FLIRT and FreeSurfer recon-all, followed by ANTs brainExtraction in single and double precision. Store bound was low for all pipelines. We note an interesting relationship between the low L1 bound and the high DRAM bound of FreeSurfer recon-all and FSL FLIRT, compared to the other pipelines.

Optimizing the data access of the pipeline could reduce the number of data load. This would potentially have a large impact on the performance of the pipelines. Alternatively, the use of reduced precision could reduce the memory footprint, thus the memory bound. To reduce the memory bound impact of fetching data from disk, it could be possible to convert and store the data to a lower precision before pre-processing. In both cases, the impact of reduced precision on the accuracy of the pipelines should be studied.
			
\csvnames{csvcol}{1=\pipeline, 2=\mem, 3=\la, 4=\lb, 5=\lc, 6=\dram, 7=\store}
\csvstyle{memory_bound}{
	tabular = |r|c|c|c|c|c|c|,
	table head = \hline Pipeline &  \% Memory Bound &  \% L1 Bound &  \% L2 Bound &  \% L3 Bound & \% DRAM Bound & \% Store Bound\\\hline\hline,
	late after line = \\\hline,
	respect all,
	csvcol
}
\begin{table*}[ht]
	\centering
	\csvreader[memory_bound]{tables/memory_bound.csv}{}
	{\pipeline & \tablenum[round-precision=2]{\mem} & \tablenum[round-precision=2]{\la} & \tablenum[round-precision=2]{\lb} & \tablenum[round-precision=2]{\lc} & \tablenum[round-precision=2]{\dram} & \tablenum[round-precision=2]{\store}}
	\caption{Impact of data load on stalled CPU cycles. For each metric (memory, L1, L2, L3, DRAM, and store), reported values are summations weighted by function CPU time, averaged over n=20 subjects, and obtained in single-threaded mode. They represent the percentage of the total CPU time stalled for the metric.}
	\label{extab:memory-bound}
\end{table*}			
						
\subsection{ANTs: Single vs. Double Precision}
Figure~\ref{fig:makespan-ants} shows the makespan of ANTs brainExtraction and ANTs registrationSyN in double and single precision. For both pipelines, the makespan was significantly higher in single precision than in double precision. This was unexpected, as the floating-point arithmetic operations are supposed to be faster in single precision~\cite{Wang2018-jv} than in double precision.

\begin{figure}[ht]
	\includegraphics[width=\linewidth]{figures/makespan-ants.png}
	\caption{Comparison of makespan between double (blue) and single (orange) precision for ANTs brainExtraction and ANTs registrationSyN.}
	\label{fig:makespan-ants}
\end{figure}

ANTs registrationSyN is a three-stage process (Euler transform, affine transform, and SyN registration), where each stage is successively performed at 4 resolution levels with a specified number of maximum iterations. We focused on the SyN registration stage since it was substantially longer to execute than the other stages. The number of iterations between both versions of ANTs registrationSyN was similar. In fact, for the last two levels, the maximum specified number of iterations was reached across all but one execution. However, the single precision version of ANTs registrationSyN took approximately 20-25\% longer per iteration than the double precision version for the last registration stage (Figure~\ref{fig:mean-time-per-iteration-ants}). Therefore, the slowdown was not due to a slower convergence of the algorithm, but rather to an increased execution time for each iteration.

After further analysis of the ANTs registrationSyN single-precision pipeline, we found that the use of templates in ANTs' C++ code\footnote{\href{https://github.com/InsightSoftwareConsortium/ITK/blob/d9c585d96359bf304ad3047148cee81bf27ac0c1/Modules/Core/ImageFunction/include/itkVectorInterpolateImageFunction.h\#L46-L48}{itkVectorInterpolateImageFunction.h}} forces the output of the interpolation function to be in double precision, with \textit{EvaluateAtContinuousIndex} contributing most of the CPU time. Therefore, the pipeline wastes CPU cycles to convert the input data to double-precision in ITK, and potentially to cast back to single-precision in ANTs. We think this bug causes the slow-down. We tried recompiling a version of ITK to fix this issue, but we were unsuccessful due to the size and complexity of the code base. This example shows that the use of reduced precision is not trivial and requires a deep understanding of the code base. Moreover, benchmarks should be performed to ensure that the reduced precision does not impact the performance or accuracy of the pipeline. We reported this issue in the ITK GitHub repository\footnote{\href{https://github.com/InsightSoftwareConsortium/ITK/issues/4593}{https://github.com/InsightSoftwareConsortium/ITK/issues/4593}}.

\begin{figure}
	\includegraphics[width=\linewidth]{figures/ants-registrationSyN-iteration-mean.png}
	\caption{Average time per iteration for ANTs registrationSyN in double and single precision. Only the SyN Registration stage is shown as the two earlier stages were near zero time. The error bars show the standard deviation across n=20 subjects.}
	\label{fig:mean-time-per-iteration-ants}
\end{figure}
						
\subsection{FreeSurfer: Thread-Synchronization}
Figure~\ref{fig:hotspots-freesurfer-reconall} shows a significant difference in the pipeline profile between single-threaded and multi-threaded executions. In the multi-threaded executions (Figure~\ref{subfig:hotspots-freesurfer-reconall-32threads}), the OpenMP multithreading library was a major bottleneck for the pipeline, accounting for at least 76\% of the pipeline runtime. FreeSurfer used the multi-processing API provided by OpenMP. Moreover, the y-axis scale is multi folds larger than for the single-threaded execution, indicating that the multi-threaded execution was significantly slower than the single-threaded execution.
					
\begin{figure*}
	\centering
	\begin{subfigure}[t]{0.49\textwidth}
		\caption{Single-threaded}
		\label{subfig:hotspots-freesurfer-reconall-1thread}
		\includegraphics[width=\textwidth]{figures/hotspots-1threads-freesurfer-reconall-simple.png}
	\end{subfigure}
	\begin{subfigure}[t]{0.49\textwidth}
		\caption{Multi-threaded (32 threads)}
		\label{subfig:hotspots-freesurfer-reconall-32threads}
		\includegraphics[width=\textwidth]{figures/hotspots-32threads-freesurfer-reconall-simple.png}
	\end{subfigure}
	\caption{FreeSurfer recon-all analysis. The y-axis shows the average CPU time spent in each function, with error bars showing the standard deviation across n=20 subjects. The x-axis shows the function ordered by decreasing CPU time grouped by module. We omitted function names for clarity. The function ID are dependent to each plot. Supplementary materials show the mapping of the function ID to the function name for each plot.}
	\label{fig:hotspots-freesurfer-reconall}
\end{figure*}
			
While the makespan for FreeSurfer recon-all decreased with an increase in the number of threads, the benefits were limited (Figure~\ref{fig:freesurfer-threading}). The parallel efficiency decreased from 66\% with two threads down to 5\% with 32 threads. While Amdhal's law limits the maximum parallel efficiency of the pipeline, Figure~\ref{fig:hotspots-freesurfer-reconall} shows that most of the CPU time is spent in libiomp, which suggests that thread synchronization had a larger impact on parallel efficiency.	
\begin{figure}
	\includegraphics[width=\linewidth]{figures/makespan-freesurfer.png}
	\caption{Makespan and parallel efficiency of FreeSurfer recon-all while varying the number of threads from 1 to 32. The left y-axis shows the makespan in seconds, while the right y-axis shows the parallel efficiency in percent. The log-scaled x-axis shows the number of threads.}
	\label{fig:freesurfer-threading}
\end{figure}

OpenMP offers multiple scheduling policies to assign chunks to threads, including \textit{static} scheduling that assigns chunks to threads in round-robin fashion, and \textit{dynamic} which assign chunks to threads as they become available for work. In the codebase, 83 out of 92 tasks use the \textit{static} scheduling policy. This could lead to a load imbalance between the threads, and thus a decrease in parallel efficiency. We speculated that \textit{dynamic} scheduling could improve the parallel efficiency, by assigning chunks based on the current load of the threads. Unfortunately, we failed to recompile FreeSurfer having changed the \textit{static} OpenMP scheduling to \textit{dynamic}. This remains challenging as the codebase is large and complex, and the change in scheduling policy could lead to unexpected bugs.
We opened an issue on FreeSurfer GitHub repository to report this problem\footnote{\href{https://github.com/freesurfer/freesurfer/issues/1199}{https://github.com/freesurfer/freesurfer/issues/1199}}.
			
Future work should study the different configuration of OpenMP scheduling in FreeSurfer recon-all to improve the parallel efficiency. We believe that significant performance improvements could be achieved by optimizing the thread synchronization. This would lead to faster runtime time and higher CPU usage ration in HPC allocations.

\subsection{Experiencing using VTune}
The VTune profiler provided a rich and extensive amount of data for our analysis, which would have been difficult to retrieve otherwise. Thanks to its low performance overhead, we were able to profile the pipeline with minimal impact on the runtime, allowing the use of more subjects to obtain a more accurate performance profile. The documentation provided by VTune is extensive and well-documented, which helped us to understand the different metrics and events available.

VTune requires a finalization step to query the data and generate a report. We found this step to be both time-consuming and resource intensive. For example, finalizing the results from a single FreeSurfer recon-all execution can require over \SI{2}{\tera\byte} of RAM, while the developers recommend to use between \SI{8}{\giga\byte} to \SI{16}{\giga\byte} of RAM to run the pipeline. This significant difference in resource requirement creates a challenge to profile pipeline with long runtime, due to the potential difficulty to obtain access to compute nodes with enough RAM. One way to mitigate this challenge is to lower the sampling rate during the profiling. However, the sampling rate would have to be significantly reduced for the finalization stage to require  \SI{16}{\giga\byte} of RAM or less. This would lead to a potential loss of information in the profiling results.
Because of these limitations, we were unable to finalize the results of the entire fMRIPrep pipeline and we only reported results for the sub-pipelines.

\section{Conclusion}
In this paper, we profiled several MRI pre-processing pipelines used in the popular fMRIPrep tool: ANTs brainExtraction, ANTs registration, FSL FAST, FSL FLIRT, FSL MCFLIRT, FreeSurfer recon-all. We observed that few functions contribute to a large amount of CPU time which offers opportunities to optimize a practical amount of functions to potentially obtain significant speed-up values. We found that most pipelines suffered from memory bound, and that linear interpolation was the primary time bottleneck. We discovered a bug in ITK which leads the implementation of ANTs registration in single-precision to use double precision instead. We also discovered a potential bug in FreeSurfer recon-all which limits the benefit from multi-threading with OpenMP. Lastly, we discussed the challenge of profiling long-running pipelines with VTune, due to computational resource requirements.

The dataset we chose contains wide range of age and equal distribution of sex. However, it only contains healthy subjects. It would be interesting to study the impact of image quality and pathologies on performance. Moreover, our profiling with VTune did not account for the spatiality (e.g. for loop) or temporality (e.g. start vs. end of convergence) of the functions. This information could provide additional insight for optimization. Future work could extend the depth of the analysis by including this information.

Future optimization efforts could focus on reduced precision techniques, to reduce memory bound and accelerate interpolation computation. We note that the use of reduced precision is not trivial, as seen with the ANTs bug. Therefore, careful attention would be required to find a balance between performance and accuracy. We also suggest that future work study the different configurations of OpenMP scheduling in FreeSurfer recon-all, to improve parallel efficiency. This would lead to faster executions time and higher CPU usage in HPC allocations.

We hope that this work serves as a reference for future work to optimize MRI pre-processing pipelines. 
			
% \TG{Move this paragraph to future work}
% Despite an extensive literature on linear interpolation \MD{TODO cite it}, no particular optimization techniques are available in the MRI pre-processing pipelines we profiled. We think it would be interesting to explore simple optimization techniques. For example, the authors in \cite{Canelhas2018-vs} review different linear interpolation methods ranging from poorly performing nearest-neighbor to classical trilinear interpolation. We think it would be interesting for future work to explore these different alternatives to accelerate interpolation in the pipelines, while considering the trade-off between accuracy and computational cost. Reduced- or mixed-precision techniques could also be explored to accelerate interpolation, with similar trade-off. Optimizing the data format used for interpolation would reduce the computational cost, memory footprint, and storage requirements. Improving the performance of interpolation would have a large impact for 3D MRI imaging processing, and other fields such as climate simulation, astronomical imaging, or 3D image reconstruction.			
\section{Data Availability}
\label{sec:data-availability}
The entire OpenNeuro ds004513 v1.0.2 dataset is freely available to download at:
\\\href{https://openneuro.org/datasets/ds004513/versions/1.0.2}{https://openneuro.org/datasets/ds004513/versions/1.0.2}.
	
The container images for our profiling experiments are available on Docker Hub:
\begin{itemize}
	\item mathdugre/cmake:debug-info
	\item mathdugre/intel-compilers:debug-info
	\item mathdugre/afni:debug-info
	\item mathdugre/ants:debug-info
	\item mathdugre/fsl:debug-info
	\item mathdugre/freesurfer:debug-info
	\item mathdugre/fmriprep:debug-info
\end{itemize}
	
Our profiling results from VTune are available at:
\\\href{https://doi.org/10.5281/zenodo.10987491}{https://doi.org/10.5281/zenodo.10987491}
\MD{TODO finalize the metadata entry and publish}

\section{Code Availability}
\label{sec:code-availability}
The code to download the data, compile and profile the pipelines, and generate the figures is available at:
\\\href{https://github.com/mathdugre/mri-bottleneck}{https://github.com/mathdugre/mri-bottleneck}
\MD{TODO Update URL to slashbin org, when created.}
													
\section*{Acknowledgement}
Mathieu Dugr\'e was jointly funded by the Natural Sciences and Engineering Research Council of Canada (NSERC) through the Postgraduate Scholarship-Doctoral (PGS-D) program and the Graduate Doctoral Fellowship Award from Concordia University. This work was partially funded by the Canada Research Chairs program. The computing infrastructure was provided by the Canada Foundation for Innovation. 
													
\section*{Conflict of Interests}
The authors report no conflict of interests.
													
\bibliographystyle{IEEEtran}
% \bibliography{IEEEabrv, paper}
\bibliography{paper}
													
\newpage
\onecolumn
\section*{Supplementary Materials}
\begin{figure*}[ht]
	\noindent
	\begin{minipage}
		{0.5\textwidth}
		\begin{align*}
			C & = CPU\_CLK\_UNHALTED.THREAD                \\
			S & = CYCLE\_ACTIVITY.STALLS\_LDM\_PENDING     \\
			O & = CYCLE\_ACTIVITY.STALLS\_L1D\_PENDING     \\
			T & = CYCLE\_ACTIVITY.STALLS\_L2\_PENDING      \\
			W & = MEM\_L3\_WEIGHT                          \\
			H & = MEM\_LOAD\_UOPS\_RETIRED.LLC\_HIT        \\
			R & = MEM\_LOAD\_UOPS\_MISC\_RETIRED.LLC\_MISS \\
			M & = \frac{W \times R}{H + R}                 \\
			N & = \frac{H}{H + W \times R}                 
		\end{align*}
	\end{minipage}
	\begin{minipage}
		{0.5\textwidth}
		\begin{align}
			\%~Memory~Bound & = \frac{S}{C} \times 100          \\
			\%~L1~Bound     & = \frac{S - O}{C} \times 100      \\
			\%~L2~Bound     & = \frac{O - T}{C} \times 100      \\
			\%~L3~Bound     & = \frac{T \times N}{C} \times 100 \\
			\%~DRAM~Bound   & = \frac{T \times M}{C} \times 100 
		\end{align}
	\end{minipage}
	\caption{Intel performance monitoring events and derived memory metrics}
	\label{fig:memory-metrics}
\end{figure*}

\MD{From Tristan: If possible, include the colorful figure in the paper.}

\MD{Not sure if these add value?}
\label{sec:supplementary}

\begin{figure*}[ht]
	\centering
	\includegraphics{figures/zipf_law.png}
	\caption{Distribution of the functions' CPU time compare to a Zipf's law distribution with $\alpha=1.46$. The y-axis shows the average CPU time for a function. The x-axis shows the percentage of functions ordered by decreasing CPU time. The data includes all functions from all pipelines.}
	\label{fig:zips-law}
\end{figure*}
													
\csvnames{csvcol}{2=\module, 3=\func, 4=\mean, 5=\std}
\csvstyle{hotspot}{
	tabular = |r|l|l|c|,
	table head = \hline & Module & Function & CPU Time (mean$\pm$std)\\\hline\hline,
	late after line = \\\hline,
	respect all,
	csvcol
}
\newcommand{\csvtable}[3]{
	\begin{table}[ht]
		\resizebox*{\textwidth}{!}{
			\csvreader[hotspot]{#1}{}
			% \csvreader[hotspot, filter={\value{csvrow}<60}]{#1}{}
			{\thecsvrow & \module & \func & \tablenum[round-precision=2, round-mode=places]{\mean}$\pm$\tablenum[round-precision=2, round-mode=places]{\std}}
		}
		\caption{#2}
		\label{#3}
	\end{table}
}
% CSV tables
% \csvtable{tables/hotspots-1thread-freesurfer-reconall.csv}{FreeSurfer recon-all (1 thread): Top functions accouting for 80\% of the pipeline makespan.}{extab:hotspots-1thread-freesurfer-reconall}
\csvtable{tables/hotspots-32threads-freesurfer-reconall.csv}{FreeSurfer recon-all (32 threads): Top functions accouting for 80\% of the pipeline makespan.}{extab:hotspots-32threads-freesurfer-reconall}
			
\end{document}
